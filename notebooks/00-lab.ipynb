{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88083606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Annotated, Any, Generator, Literal, Type, TypeVar\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "pl.Config.set_tbl_rows(n=200)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b2ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme({\n",
    "    \"white\": \"#FFFFFF\",  # Bright white\n",
    "    \"info\": \"#00FF00\",  # Bright green\n",
    "    \"warning\": \"#FFD700\",  # Bright gold\n",
    "    \"error\": \"#FF1493\",  # Deep pink\n",
    "    \"success\": \"#00FFFF\",  # Cyan\n",
    "    \"highlight\": \"#FF4500\",  # Orange-red\n",
    "})\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "\n",
    "def create_path(path: str | Path) -> None:\n",
    "    \"\"\"\n",
    "    Create parent directories for the given path if they don't exist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str | Path\n",
    "        The file path for which to create parent directories.\n",
    "\n",
    "    \"\"\"\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "142ff501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/MyProjects/batch-process\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ecde1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'person_id'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'sex'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'male'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'age'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22.0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pclass'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'sibsp'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'parch'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'fare'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.25</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'embarked'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'s'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'survived'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">46</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">265181</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'probability'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0919664529198018</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'person_id'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[32m'sex'\u001b[0m: \u001b[32m'male'\u001b[0m,\n",
       "    \u001b[32m'age'\u001b[0m: \u001b[1;36m22.0\u001b[0m,\n",
       "    \u001b[32m'pclass'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "    \u001b[32m'sibsp'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "    \u001b[32m'parch'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "    \u001b[32m'fare'\u001b[0m: \u001b[1;36m7.25\u001b[0m,\n",
       "    \u001b[32m'embarked'\u001b[0m: \u001b[32m's'\u001b[0m,\n",
       "    \u001b[32m'survived'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "    \u001b[32m'created_at'\u001b[0m: \u001b[1;35mdatetime.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m17\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[1;36m46\u001b[0m, \u001b[1;36m58\u001b[0m, \u001b[1;36m265181\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'probability'\u001b[0m: \u001b[1;36m0.0919664529198018\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'data'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'person_id'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'survived'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'probability'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'status'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'success'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'created_at'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">46</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">288549</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'data'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'person_id'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'survived'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'probability'\u001b[0m: \u001b[1;36m0.09\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'status'\u001b[0m: \u001b[32m'success'\u001b[0m,\n",
       "    \u001b[32m'created_at'\u001b[0m: \u001b[1;35mdatetime.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m17\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[1;36m46\u001b[0m, \u001b[1;36m58\u001b[0m, \u001b[1;36m288549\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from schemas import ModelOutput, PersonSchema\n",
    "from src.celery import BaseMLTask\n",
    "from src.ml.utils import _get_prediction\n",
    "\n",
    "model_dict: dict[str, Any] = BaseMLTask().model_dict\n",
    "item: dict[str, Any] = {\n",
    "    \"id\": \"0\",\n",
    "    \"sex\": \"male\",\n",
    "    \"age\": 22,\n",
    "    \"survived\": 1,\n",
    "    \"pclass\": 3,\n",
    "    \"sibsp\": 1,\n",
    "    \"parch\": 0,\n",
    "    \"fare\": 7.25,\n",
    "    \"embarked\": \"s\",\n",
    "}\n",
    "record = PersonSchema(**item)\n",
    "data_dict: dict[str, Any] = _get_prediction(record, model_dict)[0]\n",
    "\n",
    "console.print(data_dict)\n",
    "console.print(ModelOutput(**{\"data\": data_dict, \"status\": \"success\"}).model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d3b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "services:\n",
    "  local-rabbitmq: # 1st service\n",
    "    image: rabbitmq:4.1.2-management\n",
    "    container_name: local-rabbitmq # Also used as hostname\n",
    "    env_file: # Location of file(s) containing the env vars. Only accessed by the container.\n",
    "      - .env\n",
    "    ports:\n",
    "      - 5672:5672\n",
    "      - 15672:15672\n",
    "    volumes: # Persist the data volume\n",
    "      - rabbitmq-data:/var/lib/rabbitmq\n",
    "      # Volume mapping for the config file\n",
    "      # It contains the RabbitMQ configuration\n",
    "      - ./rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"rabbitmq-diagnostics\", \"check_port_connectivity\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "\n",
    "  postgres: # 2nd service\n",
    "    image: postgres:17.5-bookworm\n",
    "    # Remove name to allow Docker to automatically generate a name\n",
    "    # when you have more than one replica\n",
    "    # container_name: local-rmq-worker\n",
    "    container_name: database\n",
    "    env_file:\n",
    "      - .env\n",
    "    ports:\n",
    "      - \"5433:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data  # Bind mount for the data folder\n",
    "    depends_on:\n",
    "      local-rabbitmq:\n",
    "        condition: service_healthy\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"pg_isready -U user -d celery_db\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "\n",
    "  celery-worker: # 3rd service\n",
    "    image: celery-worker:v1\n",
    "    build:\n",
    "      context: ./\n",
    "      dockerfile: Dockerfile\n",
    "    container_name: celery-worker\n",
    "    environment:\n",
    "      - RABBITMQ_HOST=local-rabbitmq\n",
    "    env_file:\n",
    "      - .env\n",
    "    volumes:\n",
    "      - ./data:/app/data  # Bind mount for the data folder\n",
    "      - ./models:/app/models\n",
    "    cpus: \"0.5\"\n",
    "    develop:\n",
    "    # Create a `watch` configuration to update the app\n",
    "      watch:\n",
    "        - action: sync\n",
    "          path: ./\n",
    "          target: /app\n",
    "          # Folders and files to ignore\n",
    "          ignore:\n",
    "            - .venv\n",
    "            - \"**/**/*.ipynb\"\n",
    "        # Rebuild image if any of these files change\n",
    "        - action: rebuild\n",
    "          path: ./pyproject.toml\n",
    "    depends_on:\n",
    "      local-rabbitmq:\n",
    "        condition: service_healthy\n",
    "\n",
    "  celery-beat: # 4th service\n",
    "      image: celery-worker:v1\n",
    "      build:\n",
    "        context: ./\n",
    "      container_name: celery-beat\n",
    "      environment:\n",
    "      - RABBITMQ_HOST=local-rabbitmq\n",
    "      env_file:\n",
    "        - .env\n",
    "      command: uv run celery -A src.celery.app beat --loglevel=info\n",
    "      depends_on:\n",
    "        local-rabbitmq:\n",
    "          condition: service_healthy\n",
    "\n",
    "  flower: # 5th service\n",
    "      image: celery-worker:v1\n",
    "      build:\n",
    "        context: ./\n",
    "      container_name: celery-flower\n",
    "      command: uv run celery -A src.celery.app flower --basic_auth=$CELERY_FLOWER_USER:$CELERY_FLOWER_PASSWORD\n",
    "      environment:\n",
    "      - RABBITMQ_HOST=local-rabbitmq\n",
    "      env_file:\n",
    "        - .env\n",
    "      ports:\n",
    "        - \"5555:5555\"\n",
    "      depends_on:\n",
    "        local-rabbitmq:\n",
    "          condition: service_healthy\n",
    "\n",
    "# Named volumes ONLY!\n",
    "# Persist data outside the lifecycle of the container.\n",
    "volumes:\n",
    "  rabbitmq-data:\n",
    "  postgres_data:\n",
    "\n",
    "\n",
    "# Error logs\n",
    "# celery-beat:\n",
    "[2025-07-17 14:54:17,287: WARNING/MainProcess] sqlalchemy.exc\n",
    "\n",
    "[2025-07-17 14:54:17,288: WARNING/MainProcess] .\n",
    "\n",
    "[2025-07-17 14:54:17,288: WARNING/MainProcess] OperationalError\n",
    "\n",
    "[2025-07-17 14:54:17,288: WARNING/MainProcess] : \n",
    "\n",
    "[2025-07-17 14:54:17,288: WARNING/MainProcess] (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5433 failed: Connection refused\n",
    "\n",
    "\tIs the server running on that host and accepting TCP/IP connections?\n",
    "\n",
    "connection to server at \"localhost\" (127.0.0.1), port 5433 failed: Connection refused\n",
    "\n",
    "\tIs the server running on that host and accepting TCP/IP connections?\n",
    "\n",
    "\n",
    "(Background on this error at: https://sqlalche.me/e/20/e3q8)⁠\n",
    "\n",
    "celery beat v5.5.3 (immunity) is starting.\n",
    "\n",
    "__    -    ... __   -        _\n",
    "\n",
    "LocalTime -> 2025-07-17 14:54:15\n",
    "\n",
    "Configuration ->\n",
    "\n",
    "    . broker -> amqp://guest:**@localhost:5672//\n",
    "\n",
    "    . loader -> celery.loaders.app.AppLoader\n",
    "\n",
    "    . scheduler -> celery.beat.PersistentScheduler\n",
    "\n",
    "    . db -> celerybeat-schedule\n",
    "\n",
    "    . logfile -> [stderr]@%INFO\n",
    "\n",
    "    . maxinterval -> 5.00 minutes (300s)\n",
    "\n",
    "Exception ignored in: <function Shelf.__del__ at 0x7f96fada4360>\n",
    "\n",
    "Traceback (most recent call last):\n",
    "\n",
    "  File \"/usr/local/lib/python3.12/shelve.py\", line 162, in __del__\n",
    "\n",
    "  File \"/usr/local/lib/python3.12/shelve.py\", line 144, in close\n",
    "\n",
    "  File \"/usr/local/lib/python3.12/shelve.py\", line 168, in sync\n",
    "\n",
    "  File \"/usr/local/lib/python3.12/shelve.py\", line 124, in __setitem__\n",
    "\n",
    "_pickle.PicklingError: Can't pickle <class 'celery.beat.ScheduleEntry'>: import of module 'celery.beat' failed\n",
    "\n",
    "# celery-worker:\n",
    " Downloading jupyterlab\n",
    "\n",
    " Downloading notebook\n",
    "\n",
    "Installed 91 packages in 1.02s\n",
    "\n",
    "Bytecode compiled 9881 files in 13.11s\n",
    "\n",
    "2025-07-17 14:54:27 - database_utilities - [INFO] - Connected to 'dev' environment database.\n",
    "\n",
    "2025-07-17 14:54:27 - database_utilities - [INFO] - Database connection pool initialized\n",
    "\n",
    "Traceback (most recent call last):\n",
    "\n",
    "  File \"/app/.venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py\", line 145, in __init__\n",
    "\n",
    "    self._dbapi_connection = engine.raw_connection()\n",
    "\n",
    "# ==== TRUNCATED ====\n",
    "\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "  File \"/app/.venv/lib/python3.12/site-packages/psycopg2/__init__.py\", line 122, in connect\n",
    "\n",
    "    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n",
    "\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5433 failed: Connection refused\n",
    "\n",
    "\tIs the server running on that host and accepting TCP/IP connections?\n",
    "\n",
    "connection to server at \"localhost\" (127.0.0.1), port 5433 failed: Connection refused\n",
    "\n",
    "\tIs the server running on that host and accepting TCP/IP connections?\n",
    "\n",
    "\n",
    "(Background on this error at: https://sqlalche.me/e/20/e3q8)⁠\n",
    "\n",
    "# flower:\n",
    "Bytecode compiled 9881 files in 1.22s\n",
    "\n",
    "[I 250717 14:54:04 command:168] Visit me at http://0.0.0.0:5555⁠\n",
    "\n",
    "[I 250717 14:54:04 command:176] Broker: amqp://guest:**@localhost:5672//\n",
    "\n",
    "[I 250717 14:54:05 command:177] Registered tasks: \n",
    "\n",
    "    ['celery.accumulate',\n",
    "\n",
    "     'celery.backend_cleanup',\n",
    "\n",
    "     'celery.chain',\n",
    "\n",
    "     'celery.chord',\n",
    "\n",
    "     'celery.chord_unlock',\n",
    "\n",
    "     'celery.chunks',\n",
    "\n",
    "     'celery.group',\n",
    "\n",
    "     'celery.map',\n",
    "\n",
    "     'celery.starmap',\n",
    "\n",
    "     'src.celery.tasks.data_processing.combine_processed_chunks',\n",
    "\n",
    "     'src.celery.tasks.data_processing.process_data_chunk',\n",
    "\n",
    "     'src.celery.tasks.data_processing.process_large_dataset',\n",
    "\n",
    "     'src.celery.tasks.email_tasks.send_bulk_emails',\n",
    "\n",
    "     'src.celery.tasks.email_tasks.send_email',\n",
    "\n",
    "     'src.celery.tasks.ml_prediction_tasks.combine_processed_chunks',\n",
    "\n",
    "     'src.celery.tasks.ml_prediction_tasks.ml_process_large_dataset',\n",
    "\n",
    "     'src.celery.tasks.ml_prediction_tasks.process_data_chunk',\n",
    "\n",
    "     'src.celery.tasks.periodic_tasks.cleanup_old_records',\n",
    "\n",
    "     'src.celery.tasks.periodic_tasks.health_check']\n",
    "\n",
    "[E 250717 14:54:11 events:191] Failed to capture events: '[Errno 111] Connection refused', trying again in 2 seconds.\n",
    "\n",
    "[E 250717 14:54:11 base_events:1833] Future exception was never retrieved\n",
    "\n",
    "    future: <Future finished exception=OperationalError('[Errno 111] Connection refused')>\n",
    "\n",
    "    Traceback (most recent call last):\n",
    "\n",
    "      File \"/app/.venv/lib/python3.12/site-packages/kombu/connection.py\", line 472, in _reraise_as_library_errors\n",
    "\n",
    "        yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068ded2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace37a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Define a function to generate a random id\n",
    "def generate_random_id(length: int = 8) -> str:\n",
    "    \"\"\"\n",
    "    Generate a random id string of a given length.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int, optional\n",
    "        Length of the id string to generate. Defaults to 8.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A random id string of the given length.\n",
    "    \"\"\"\n",
    "    return \"\".join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "\n",
    "# Define a function to generate a list of random person data\n",
    "def generate_person_data(num_entries: int) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a list of random person data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_entries : int\n",
    "        Number of person data entries to generate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    person_data : list[dict]\n",
    "        List of dictionaries, each containing person data.\n",
    "    \"\"\"\n",
    "    sex_options: list[Literal[\"male\", \"female\"]] = [\"male\", \"female\"]\n",
    "    embarked_options: list[Literal[\"s\", \"c\", \"q\"]] = [\"s\", \"c\", \"q\"]\n",
    "\n",
    "    person_data = []\n",
    "\n",
    "    for _ in range(num_entries):\n",
    "        person = {\n",
    "            \"personId\": generate_random_id(),\n",
    "            \"sex\": random.choice(sex_options),\n",
    "            \"age\": round(random.uniform(0.5, 80.0), 2),\n",
    "            \"pclass\": random.randint(1, 3),\n",
    "            \"sibsp\": random.randint(0, 5),\n",
    "            \"parch\": random.randint(0, 5),\n",
    "            \"fare\": round(random.uniform(5.0, 200.0), 2),\n",
    "            \"embarked\": random.choice(embarked_options),\n",
    "        }\n",
    "        person_data.append(person)\n",
    "\n",
    "    return person_data\n",
    "\n",
    "\n",
    "# Generate a list of 10 random person data entries\n",
    "person_data_list = generate_person_data(10)\n",
    "print(person_data_list)\n",
    "\n",
    "fp: str = \"./data/sample_data.jsonl\"\n",
    "\n",
    "with open(fp, \"w\") as f:\n",
    "    for person in person_data_list:\n",
    "        f.write(json.dumps(person) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba7aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import delete, insert, select, update\n",
    "\n",
    "from schemas import EmailSchema\n",
    "from src.database.db_models import EmailLog, get_db_session, init_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72537b8c",
   "metadata": {},
   "source": [
    "## [Docs](https://docs.sqlalchemy.org/en/20/orm/queryguide/select.html)\n",
    "\n",
    "### [Insert](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-bulk-insert-statements)\n",
    "\n",
    "- Old API\n",
    "\n",
    "```python\n",
    "with get_db_session() as session:\n",
    "    data_dict = input_data.model_dump()\n",
    "    record = EmailLog(**data_dict)\n",
    "    session.add(record)\n",
    "    session.flush()\n",
    "    output_data = {key: getattr(record, key) for key in record.output_fields()}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- New API\n",
    "\n",
    "```py\n",
    "with get_db_session() as session:\n",
    "    data_dict = input_data.model_dump()\n",
    "    session.execute(insert(EmailLog), [data_dict])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data: EmailSchema = EmailSchema(\n",
    "    recipient=\"marketing@client.com\",\n",
    "    subject=\"Partnership Proposal\",\n",
    "    body=\"We would like to discuss a potential partnership opportunity.\",\n",
    ")\n",
    "console.print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    data_dict = input_data.model_dump()\n",
    "    record = EmailLog(**data_dict)\n",
    "    session.add(record)\n",
    "    session.flush()\n",
    "    output_data = {key: getattr(record, key) for key in record.output_fields()}\n",
    "\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cf44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    statement = session.query(EmailLog).where(EmailLog.created_at < datetime.now())\n",
    "    record = session.execute(statement).scalar_one()\n",
    "    output_data = {key: getattr(record, key) for key in record.output_fields()}\n",
    "\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f60984",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_2: EmailSchema = EmailSchema(\n",
    "    recipient=\"emeka2@example.com\",\n",
    "    subject=\"test!!!\",\n",
    "    body=\"this is an example body\",\n",
    "    status=\"processing\",\n",
    ")\n",
    "input_data_3: EmailSchema = EmailSchema(\n",
    "    recipient=\"john.doe@example.com\",\n",
    "    subject=\"Meeting Reminder\",\n",
    "    body=\"Hi John, just a reminder about our meeting tomorrow at 10 AM.\",\n",
    "    status=\"processing\",\n",
    ")\n",
    "input_data_4: EmailSchema = EmailSchema(\n",
    "    recipient=\"info@company.org\",\n",
    "    subject=\"New Product Launch\",\n",
    "    body=\"Dear valued customer, check out our exciting new product!\",\n",
    "    status=\"sent\",\n",
    "    created_at=datetime(2025, 7, 10, 9, 0, 0),\n",
    "    sent_at=\"2025-07-10T09:05:00\",\n",
    ")\n",
    "console.print((input_data_2, input_data_3, input_data_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321d8cd",
   "metadata": {},
   "source": [
    "### [Bulk Insert](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-bulk-insert-statements)\n",
    "\n",
    "- Old API\n",
    "\n",
    "```py\n",
    "with get_db_session() as session:\n",
    "    data_list: list[dict[str, Any]] = [_data.to_data_model_dict() for _data in (input_data_2, input_data_3, input_data_4)]\n",
    "    session.bulk_insert_mappings(EmailLog, data_list)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- New API\n",
    "\n",
    "```py\n",
    "with get_db_session() as session:\n",
    "    data_list: list[dict[str, Any]] = [\n",
    "        _data.to_data_model_dict()\n",
    "        for _data in (input_data_2, input_data_3, input_data_4)\n",
    "    ]\n",
    "    session.execute(insert(EmailLog), data_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    data_list: list[dict[str, Any]] = [_data.model_dump() for _data in (input_data_2, input_data_3, input_data_4)]\n",
    "    session.execute(insert(EmailLog), data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81030c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32dc5d45",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab00eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single record\n",
    "with get_db_session() as session:\n",
    "    statement = select(EmailLog).where(EmailLog.id == 1, EmailLog.status == \"pending\")\n",
    "    record = session.execute(statement).scalar_one()\n",
    "    output_data = {key: getattr(record, key) for key in record.output_fields()}\n",
    "\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7778e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all records\n",
    "with get_db_session() as session:\n",
    "    statement = select(EmailLog)\n",
    "    record = session.execute(statement).scalars()\n",
    "\n",
    "    output_data = [{key: getattr(row, key) for key in row.output_fields()} for row in record]\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2e6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "123d4da2",
   "metadata": {},
   "source": [
    "### [Update](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-update-and-delete-with-custom-where-criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22335d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    statement = (\n",
    "        update(EmailLog)\n",
    "        .where(EmailLog.id == 1)\n",
    "        .values(status=\"sent\", sent_at=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    )\n",
    "    # It closes the session and returns None\n",
    "    session.execute(statement)\n",
    "\n",
    "# Verify that the record was updated\n",
    "with get_db_session() as session:\n",
    "    statement = select(EmailLog)\n",
    "    record = session.execute(statement).scalars()\n",
    "\n",
    "    output_data = [{key: getattr(row, key) for key in row.output_fields()} for row in record]\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecabd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90f5ae1",
   "metadata": {},
   "source": [
    "### [Delete](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-update-and-delete-with-custom-where-criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    statement = delete(EmailLog).where(EmailLog.id == 2)\n",
    "    # It closes the session and returns None\n",
    "    session.execute(statement)\n",
    "\n",
    "# Verify that the record was updated\n",
    "with get_db_session() as session:\n",
    "    statement = select(EmailLog)\n",
    "    record = session.execute(statement).scalars()\n",
    "\n",
    "    output_data = [{key: getattr(row, key) for key in row.output_fields()} for row in record]\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ee946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import app_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "beat_dict: dict[str, dict[str, Any]] = dict(app_config.celery_config.beat_config.beat_schedule.model_dump().items())\n",
    "\n",
    "# Add the health_check\n",
    "beat_dict[\"health_check\"] = app_config.celery_config.beat_config.health_check.model_dump()\n",
    "\n",
    "\n",
    "console.print(beat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_config.celery_config.beat_config.beat_schedule.model_dump().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07da3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2050af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel, field_serializer\n",
    "\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    role: str\n",
    "    salary: float = 0.0\n",
    "    others: Any | None = None\n",
    "\n",
    "    @field_serializer(\"others\")\n",
    "    def serialize(self, value: Any) -> str:\n",
    "        if isinstance(value, datetime):\n",
    "            return value.isoformat()\n",
    "        return json.dumps(value)\n",
    "\n",
    "\n",
    "def my_func(name: str, **kwargs) -> MyModel:\n",
    "    my_dict = {\"name\": name, **kwargs}\n",
    "    return MyModel(**my_dict)\n",
    "\n",
    "\n",
    "result = my_func(\n",
    "    \"Neidu\",\n",
    "    age=30,\n",
    "    role=\"AI Engineer\",\n",
    "    friend=\"None\",\n",
    "    others=[\"Hi\"],\n",
    "    # others=datetime.now(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.model_dump())\n",
    "\n",
    "json.loads(result.model_dump()[\"others\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import joblib\n",
    "\n",
    "from celery import chord, current_task, group\n",
    "from schemas import ModelOutput, MultiPersonsSchema, MultiPredOutput, PersonSchema\n",
    "from src import PACKAGE_PATH, create_logger\n",
    "from src.celery import celery_app\n",
    "from src.database import get_db_session\n",
    "from src.database.db_models import BaseTask, MLPredictionJob\n",
    "from src.ml.utils import get_batch_prediction, get_prediction\n",
    "\n",
    "logger = create_logger(name=\"ml_prediction\")\n",
    "\n",
    "\n",
    "@celery_app.task(bind=True, base=BaseTask)\n",
    "def process_prediction_chunk(self, persons_data: list[dict[str, Any]], chunk_id: int) -> dict[str, Any]:  # noqa: ANN001\n",
    "    \"\"\"\n",
    "    Process a chunk of ML predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    persons_data : list[dict[str, Any]]\n",
    "        List of person data dictionaries for prediction\n",
    "    chunk_id : int\n",
    "        Unique identifier for this chunk\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing chunk processing results and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Validate input data\n",
    "        multi_persons = MultiPersonsSchema(persons=persons_data)  # type: ignore\n",
    "        total_items = len(multi_persons.persons)\n",
    "\n",
    "        # Load model once for the entire chunk\n",
    "        model_dict_fp: Path = PACKAGE_PATH / \"models/model.pkl\"\n",
    "        with open(model_dict_fp, \"rb\") as f:\n",
    "            model_dict = joblib.load(f)\n",
    "\n",
    "        # Process predictions\n",
    "        prediction_results = []\n",
    "\n",
    "        for i, person in enumerate(multi_persons.persons):\n",
    "            # Update task progress\n",
    "            current_task.update_state(\n",
    "                state=\"PROGRESS\",\n",
    "                meta={\"current\": i + 1, \"total\": total_items, \"chunk_id\": chunk_id},\n",
    "            )\n",
    "\n",
    "            # Make individual prediction\n",
    "            result: ModelOutput = get_prediction(person, model_dict)\n",
    "            prediction_results.append(result.model_dump())\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "\n",
    "        logger.info(f\"Processed chunk {chunk_id} with {total_items} predictions in {processing_time:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"prediction_results\": prediction_results,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"items_count\": total_items,\n",
    "            \"status\": \"success\",\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing prediction chunk {chunk_id}: {e}\")\n",
    "        raise self.retry(exc=e) from e\n",
    "\n",
    "\n",
    "@celery_app.task\n",
    "def combine_prediction_results(chunk_results: list[dict[str, Any]]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Combine results from multiple prediction chunks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_results : list[dict[str, Any]]\n",
    "        List of chunk processing results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing combined prediction results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with get_db_session() as session:\n",
    "            # Sort chunks by chunk_id\n",
    "            sorted_results = sorted(chunk_results, key=lambda x: x[\"chunk_id\"])\n",
    "\n",
    "            # Combine all prediction results\n",
    "            combined_predictions = []\n",
    "            total_processing_time = 0\n",
    "            total_items = 0\n",
    "\n",
    "            for result in sorted_results:\n",
    "                combined_predictions.extend(result[\"prediction_results\"])\n",
    "                total_processing_time += result[\"processing_time\"]\n",
    "                total_items += result[\"items_count\"]\n",
    "\n",
    "            avg_processing_time = round((total_processing_time / len(sorted_results)), 2)\n",
    "\n",
    "            # Save to database\n",
    "            job_data = {\n",
    "                \"job_name\": \"batch_ml_prediction\",\n",
    "                \"input_data\": json.dumps({\"chunks\": len(sorted_results), \"total_items\": total_items}),\n",
    "                \"output_data\": json.dumps({\"predictions\": combined_predictions}),\n",
    "                \"processing_time\": avg_processing_time,\n",
    "                \"prediction_count\": total_items,\n",
    "                \"status\": \"completed\",\n",
    "                \"completed_at\": datetime.now(),\n",
    "            }\n",
    "\n",
    "            job = MLPredictionJob(**job_data)\n",
    "            session.add(job)\n",
    "            session.flush()\n",
    "\n",
    "            logger.info(f\"Combined {len(sorted_results)} chunks with {total_items} total predictions\")\n",
    "\n",
    "            return {\n",
    "                \"status\": \"completed\",\n",
    "                \"total_chunks\": len(sorted_results),\n",
    "                \"total_predictions\": total_items,\n",
    "                \"avg_processing_time\": avg_processing_time,\n",
    "                \"job_id\": job.id,\n",
    "                \"predictions\": combined_predictions,\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error combining prediction results: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "@celery_app.task\n",
    "def process_batch_predictions(persons_data: list[dict[str, Any]], chunk_size: int = 10) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a large batch of ML predictions by splitting into chunks and using chord.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    persons_data : list[dict[str, Any]]\n",
    "        List of person data dictionaries for prediction\n",
    "    chunk_size : int, optional\n",
    "        Size of each processing chunk, by default 10\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing batch processing dispatch information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Split data into chunks\n",
    "        chunks = [persons_data[i : i + chunk_size] for i in range(0, len(persons_data), chunk_size)]\n",
    "\n",
    "        # Create a chord: process chunks in parallel, then combine results\n",
    "        job = chord(\n",
    "            group(process_prediction_chunk.s(chunk, i) for i, chunk in enumerate(chunks)),\n",
    "            combine_prediction_results.s(),\n",
    "        )\n",
    "\n",
    "        result = job.apply_async()\n",
    "\n",
    "        logger.info(f\"Dispatched batch prediction job with {len(persons_data)} items in {len(chunks)} chunks\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"dispatched\",\n",
    "            \"total_items\": len(persons_data),\n",
    "            \"chunks\": len(chunks),\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"chord_id\": result.id,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error dispatching batch predictions: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "@celery_app.task(bind=True, base=BaseTask)\n",
    "def process_dlq_message(self, message_data: dict[str, Any]) -> dict[str, Any]:  # noqa: ANN001\n",
    "    \"\"\"\n",
    "    Process a message from the dead letter queue.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    message_data : dict[str, Any]\n",
    "        Message data from DLQ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing DLQ processing results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate the message data\n",
    "        if \"persons\" in message_data:\n",
    "            # Batch message\n",
    "            record = MultiPersonsSchema(**message_data)\n",
    "            message_type = \"batch\"\n",
    "            item_count = len(record.persons)\n",
    "        else:\n",
    "            # Single message\n",
    "            record = PersonSchema(**message_data)\n",
    "            message_type = \"single\"\n",
    "            item_count = 1\n",
    "\n",
    "        # Log DLQ message to database (you might want to create a DLQ table)\n",
    "        logger.warning(f\"Processing DLQ message: {message_type} with {item_count} items\")\n",
    "\n",
    "        # For now, just log the DLQ data - you can extend this to save to a DLQ table\n",
    "        with get_db_session() as session:\n",
    "            job_data = {\n",
    "                \"job_name\": f\"dlq_{message_type}_processing\",\n",
    "                \"input_data\": json.dumps(message_data),\n",
    "                \"output_data\": json.dumps({\"status\": \"dlq_processed\", \"message_type\": message_type}),\n",
    "                \"processing_time\": 0.0,\n",
    "                \"prediction_count\": 0,\n",
    "                \"status\": \"dlq_processed\",\n",
    "                \"completed_at\": datetime.now(),\n",
    "            }\n",
    "\n",
    "            job = MLPredictionJob(**job_data)\n",
    "            session.add(job)\n",
    "            session.flush()\n",
    "\n",
    "            logger.info(f\"DLQ message processed and logged with job_id: {job.id}\")\n",
    "\n",
    "            return {\n",
    "                \"status\": \"dlq_processed\",\n",
    "                \"message_type\": message_type,\n",
    "                \"item_count\": item_count,\n",
    "                \"job_id\": job.id,\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing DLQ message: {e}\")\n",
    "        raise self.retry(exc=e) from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eaaad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769141e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batch-process",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
