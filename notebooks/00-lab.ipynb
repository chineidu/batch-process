{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88083606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Annotated, Any, Generator, Literal, Type, TypeVar\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "pl.Config.set_tbl_rows(n=200)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme({\n",
    "    \"white\": \"#FFFFFF\",  # Bright white\n",
    "    \"info\": \"#00FF00\",  # Bright green\n",
    "    \"warning\": \"#FFD700\",  # Bright gold\n",
    "    \"error\": \"#FF1493\",  # Deep pink\n",
    "    \"success\": \"#00FFFF\",  # Cyan\n",
    "    \"highlight\": \"#FF4500\",  # Orange-red\n",
    "})\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "\n",
    "def create_path(path: str | Path) -> None:\n",
    "    \"\"\"\n",
    "    Create parent directories for the given path if they don't exist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str | Path\n",
    "        The file path for which to create parent directories.\n",
    "\n",
    "    \"\"\"\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "142ff501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/MyProjects/batch-process\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1ecde1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      1\u001b[39m {\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtask_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msrc.celery_pkg.tasks.ml_prediction_tasks.process_single_data\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m\u001b[33mperson_id\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mq0bPCRQH\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33msurvived\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: 1, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mprobability\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: 0.87}, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcreated_at\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[33m2025-07-17T22:15:46.385769\u001b[39m\u001b[33m'\u001b[39m\u001b[33m}, \u001b[39m\u001b[33m'\u001b[39m\u001b[33msent_at\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[33m2025-07-17T22:15:46.386249\u001b[39m\u001b[33m'\u001b[39m\u001b[33m}\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      5\u001b[39m         {\n\u001b[32m      6\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mperson_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mq0bPCRQH\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msex\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfemale\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m66.48\u001b[39m,\n\u001b[32m      9\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpclass\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m2\u001b[39m,\n\u001b[32m     10\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msibsp\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m     11\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparch\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m     12\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfare\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m149.22\u001b[39m,\n\u001b[32m     13\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33membarked\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mq\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msurvived\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcreated_at\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mdatetime\u001b[49m.datetime(\u001b[32m2025\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m17\u001b[39m, \u001b[32m22\u001b[39m, \u001b[32m15\u001b[39m, \u001b[32m46\u001b[39m, \u001b[32m92399\u001b[39m),\n\u001b[32m     16\u001b[39m         }\n\u001b[32m     17\u001b[39m     ],\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mkwargs\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m     19\u001b[39m }\n",
      "\u001b[31mNameError\u001b[39m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "{\n",
    "    \"task_name\": \"src.celery_pkg.tasks.ml_prediction_tasks.process_single_data\",\n",
    "    \"result\": \"{'status': 'success', 'response': {'data': {'person_id': 'q0bPCRQH', 'survived': 1, 'probability': 0.87}, 'status': 'success', 'created_at': '2025-07-17T22:15:46.385769'}, 'sent_at': '2025-07-17T22:15:46.386249'}\",\n",
    "    \"args\": [\n",
    "        {\n",
    "            \"person_id\": \"q0bPCRQH\",\n",
    "            \"sex\": \"female\",\n",
    "            \"age\": 66.48,\n",
    "            \"pclass\": 2,\n",
    "            \"sibsp\": 0,\n",
    "            \"parch\": 0,\n",
    "            \"fare\": 149.22,\n",
    "            \"embarked\": \"q\",\n",
    "            \"survived\": 0,\n",
    "            \"created_at\": datetime.datetime(2025, 7, 17, 22, 15, 46, 92399),\n",
    "        }\n",
    "    ],\n",
    "    \"kwargs\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"task_name\": \"src.celery_pkg.tasks.ml_prediction_tasks.process_single_data\",\n",
    "    \"result\": \"{'status': 'success', 'response': {'data': {'person_id': 'q0bPCRQH', 'survived': 1, 'probability': 0.87}, 'status': 'success', 'created_at': '2025-07-17T22:22:50.406151'}, 'sent_at': '2025-07-17T22:22:50.406610'}\",\n",
    "    \"args\": [\n",
    "        {\n",
    "            \"person_id\": \"q0bPCRQH\",\n",
    "            \"sex\": \"female\",\n",
    "            \"age\": 66.48,\n",
    "            \"pclass\": 2,\n",
    "            \"sibsp\": 0,\n",
    "            \"parch\": 0,\n",
    "            \"fare\": 149.22,\n",
    "            \"embarked\": \"q\",\n",
    "            \"survived\": 0,\n",
    "            \"created_at\": \"2025-07-17T22:22:50.104828\",\n",
    "        }\n",
    "    ],\n",
    "    \"kwargs\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace37a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Define a function to generate a random id\n",
    "def generate_random_id(length: int = 8) -> str:\n",
    "    \"\"\"\n",
    "    Generate a random id string of a given length.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int, optional\n",
    "        Length of the id string to generate. Defaults to 8.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A random id string of the given length.\n",
    "    \"\"\"\n",
    "    return \"\".join(random.choices(string.ascii_letters + string.digits, k=length))\n",
    "\n",
    "\n",
    "# Define a function to generate a list of random person data\n",
    "def generate_person_data(num_entries: int) -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a list of random person data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_entries : int\n",
    "        Number of person data entries to generate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    person_data : list[dict]\n",
    "        List of dictionaries, each containing person data.\n",
    "    \"\"\"\n",
    "    sex_options: list[Literal[\"male\", \"female\"]] = [\"male\", \"female\"]\n",
    "    embarked_options: list[Literal[\"s\", \"c\", \"q\"]] = [\"s\", \"c\", \"q\"]\n",
    "\n",
    "    person_data = []\n",
    "\n",
    "    for _ in range(num_entries):\n",
    "        person = {\n",
    "            \"personId\": generate_random_id(),\n",
    "            \"sex\": random.choice(sex_options),\n",
    "            \"age\": round(random.uniform(0.5, 80.0), 2),\n",
    "            \"pclass\": random.randint(1, 3),\n",
    "            \"sibsp\": random.randint(0, 5),\n",
    "            \"parch\": random.randint(0, 5),\n",
    "            \"fare\": round(random.uniform(5.0, 200.0), 2),\n",
    "            \"embarked\": random.choice(embarked_options),\n",
    "        }\n",
    "        person_data.append(person)\n",
    "\n",
    "    return person_data\n",
    "\n",
    "\n",
    "# Generate a list of 10 random person data entries\n",
    "person_data_list = generate_person_data(10)\n",
    "print(person_data_list)\n",
    "\n",
    "fp: str = \"./data/sample_data.jsonl\"\n",
    "\n",
    "with open(fp, \"w\") as f:\n",
    "    for person in person_data_list:\n",
    "        f.write(json.dumps(person) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba7aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import delete, insert, select, update\n",
    "\n",
    "from schemas import EmailSchema\n",
    "from src.database.db_models import EmailLog, get_db_session, init_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72537b8c",
   "metadata": {},
   "source": [
    "## [Docs](https://docs.sqlalchemy.org/en/20/orm/queryguide/select.html)\n",
    "\n",
    "### [Insert](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-bulk-insert-statements)\n",
    "\n",
    "- Old API\n",
    "\n",
    "```python\n",
    "with get_db_session() as session:\n",
    "    data_dict = input_data.model_dump()\n",
    "    record = EmailLog(**data_dict)\n",
    "    session.add(record)\n",
    "    session.flush()\n",
    "    output_data = {key: getattr(record, key) for key in record.output_fields()}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- New API\n",
    "\n",
    "```py\n",
    "with get_db_session() as session:\n",
    "    data_dict = input_data.model_dump()\n",
    "    session.execute(insert(EmailLog), [data_dict])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data: EmailSchema = EmailSchema(\n",
    "    recipient=\"marketing@client.com\",\n",
    "    subject=\"Partnership Proposal\",\n",
    "    body=\"We would like to discuss a potential partnership opportunity.\",\n",
    ")\n",
    "console.print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    data_dict = input_data.model_dump()\n",
    "    record = EmailLog(**data_dict)\n",
    "    session.add(record)\n",
    "    session.flush()\n",
    "    output_data = {key: getattr(record, key) for key in record.output_fields()}\n",
    "\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cf44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    statement = session.query(EmailLog).where(EmailLog.created_at < datetime.now())\n",
    "    record = session.execute(statement).scalar_one()\n",
    "    output_data = {key: getattr(record, key) for key in record.output_fields()}\n",
    "\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f60984",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_2: EmailSchema = EmailSchema(\n",
    "    recipient=\"emeka2@example.com\",\n",
    "    subject=\"test!!!\",\n",
    "    body=\"this is an example body\",\n",
    "    status=\"processing\",\n",
    ")\n",
    "input_data_3: EmailSchema = EmailSchema(\n",
    "    recipient=\"john.doe@example.com\",\n",
    "    subject=\"Meeting Reminder\",\n",
    "    body=\"Hi John, just a reminder about our meeting tomorrow at 10 AM.\",\n",
    "    status=\"processing\",\n",
    ")\n",
    "input_data_4: EmailSchema = EmailSchema(\n",
    "    recipient=\"info@company.org\",\n",
    "    subject=\"New Product Launch\",\n",
    "    body=\"Dear valued customer, check out our exciting new product!\",\n",
    "    status=\"sent\",\n",
    "    created_at=datetime(2025, 7, 10, 9, 0, 0),\n",
    "    sent_at=\"2025-07-10T09:05:00\",\n",
    ")\n",
    "console.print((input_data_2, input_data_3, input_data_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321d8cd",
   "metadata": {},
   "source": [
    "### [Bulk Insert](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-bulk-insert-statements)\n",
    "\n",
    "- Old API\n",
    "\n",
    "```py\n",
    "with get_db_session() as session:\n",
    "    data_list: list[dict[str, Any]] = [_data.to_data_model_dict() for _data in (input_data_2, input_data_3, input_data_4)]\n",
    "    session.bulk_insert_mappings(EmailLog, data_list)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- New API\n",
    "\n",
    "```py\n",
    "with get_db_session() as session:\n",
    "    data_list: list[dict[str, Any]] = [\n",
    "        _data.to_data_model_dict()\n",
    "        for _data in (input_data_2, input_data_3, input_data_4)\n",
    "    ]\n",
    "    session.execute(insert(EmailLog), data_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    data_list: list[dict[str, Any]] = [_data.model_dump() for _data in (input_data_2, input_data_3, input_data_4)]\n",
    "    session.execute(insert(EmailLog), data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81030c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32dc5d45",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab00eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single record\n",
    "with get_db_session() as session:\n",
    "    statement = select(EmailLog).where(EmailLog.id == 1, EmailLog.status == \"pending\")\n",
    "    record = session.execute(statement).scalar_one()\n",
    "    output_data = {key: getattr(record, key) for key in record.output_fields()}\n",
    "\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7778e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all records\n",
    "with get_db_session() as session:\n",
    "    statement = select(EmailLog)\n",
    "    record = session.execute(statement).scalars()\n",
    "\n",
    "    output_data = [{key: getattr(row, key) for key in row.output_fields()} for row in record]\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2e6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "123d4da2",
   "metadata": {},
   "source": [
    "### [Update](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-update-and-delete-with-custom-where-criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22335d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    statement = (\n",
    "        update(EmailLog)\n",
    "        .where(EmailLog.id == 1)\n",
    "        .values(status=\"sent\", sent_at=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    )\n",
    "    # It closes the session and returns None\n",
    "    session.execute(statement)\n",
    "\n",
    "# Verify that the record was updated\n",
    "with get_db_session() as session:\n",
    "    statement = select(EmailLog)\n",
    "    record = session.execute(statement).scalars()\n",
    "\n",
    "    output_data = [{key: getattr(row, key) for key in row.output_fields()} for row in record]\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecabd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90f5ae1",
   "metadata": {},
   "source": [
    "### [Delete](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-update-and-delete-with-custom-where-criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    statement = delete(EmailLog).where(EmailLog.id == 2)\n",
    "    # It closes the session and returns None\n",
    "    session.execute(statement)\n",
    "\n",
    "# Verify that the record was updated\n",
    "with get_db_session() as session:\n",
    "    statement = select(EmailLog)\n",
    "    record = session.execute(statement).scalars()\n",
    "\n",
    "    output_data = [{key: getattr(row, key) for key in row.output_fields()} for row in record]\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ee946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import app_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "beat_dict: dict[str, dict[str, Any]] = dict(app_config.celery_config.beat_config.beat_schedule.model_dump().items())\n",
    "\n",
    "# Add the health_check\n",
    "beat_dict[\"health_check\"] = app_config.celery_config.beat_config.health_check.model_dump()\n",
    "\n",
    "\n",
    "console.print(beat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_config.celery_config.beat_config.beat_schedule.model_dump().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07da3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2050af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel, field_serializer\n",
    "\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    role: str\n",
    "    salary: float = 0.0\n",
    "    others: Any | None = None\n",
    "\n",
    "    @field_serializer(\"others\")\n",
    "    def serialize(self, value: Any) -> str:\n",
    "        if isinstance(value, datetime):\n",
    "            return value.isoformat()\n",
    "        return json.dumps(value)\n",
    "\n",
    "\n",
    "def my_func(name: str, **kwargs) -> MyModel:\n",
    "    my_dict = {\"name\": name, **kwargs}\n",
    "    return MyModel(**my_dict)\n",
    "\n",
    "\n",
    "result = my_func(\n",
    "    \"Neidu\",\n",
    "    age=30,\n",
    "    role=\"AI Engineer\",\n",
    "    friend=\"None\",\n",
    "    others=[\"Hi\"],\n",
    "    # others=datetime.now(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.model_dump())\n",
    "\n",
    "json.loads(result.model_dump()[\"others\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import joblib\n",
    "from celery import chord, current_task, group\n",
    "\n",
    "from schemas import ModelOutput, MultiPersonsSchema, MultiPredOutput, PersonSchema\n",
    "from src import PACKAGE_PATH, create_logger\n",
    "from src.celery import celery_app\n",
    "from src.database import get_db_session\n",
    "from src.database.db_models import BaseTask, MLPredictionJob\n",
    "from src.ml.utils import get_batch_prediction, get_prediction\n",
    "\n",
    "logger = create_logger(name=\"ml_prediction\")\n",
    "\n",
    "\n",
    "@celery_app.task(bind=True, base=BaseTask)\n",
    "def process_prediction_chunk(self, persons_data: list[dict[str, Any]], chunk_id: int) -> dict[str, Any]:  # noqa: ANN001\n",
    "    \"\"\"\n",
    "    Process a chunk of ML predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    persons_data : list[dict[str, Any]]\n",
    "        List of person data dictionaries for prediction\n",
    "    chunk_id : int\n",
    "        Unique identifier for this chunk\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing chunk processing results and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Validate input data\n",
    "        multi_persons = MultiPersonsSchema(persons=persons_data)  # type: ignore\n",
    "        total_items = len(multi_persons.persons)\n",
    "\n",
    "        # Load model once for the entire chunk\n",
    "        model_dict_fp: Path = PACKAGE_PATH / \"models/model.pkl\"\n",
    "        with open(model_dict_fp, \"rb\") as f:\n",
    "            model_dict = joblib.load(f)\n",
    "\n",
    "        # Process predictions\n",
    "        prediction_results = []\n",
    "\n",
    "        for i, person in enumerate(multi_persons.persons):\n",
    "            # Update task progress\n",
    "            current_task.update_state(\n",
    "                state=\"PROGRESS\",\n",
    "                meta={\"current\": i + 1, \"total\": total_items, \"chunk_id\": chunk_id},\n",
    "            )\n",
    "\n",
    "            # Make individual prediction\n",
    "            result: ModelOutput = get_prediction(person, model_dict)\n",
    "            prediction_results.append(result.model_dump())\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "\n",
    "        logger.info(f\"Processed chunk {chunk_id} with {total_items} predictions in {processing_time:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"prediction_results\": prediction_results,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"items_count\": total_items,\n",
    "            \"status\": \"success\",\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing prediction chunk {chunk_id}: {e}\")\n",
    "        raise self.retry(exc=e) from e\n",
    "\n",
    "\n",
    "@celery_app.task\n",
    "def combine_prediction_results(chunk_results: list[dict[str, Any]]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Combine results from multiple prediction chunks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_results : list[dict[str, Any]]\n",
    "        List of chunk processing results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing combined prediction results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with get_db_session() as session:\n",
    "            # Sort chunks by chunk_id\n",
    "            sorted_results = sorted(chunk_results, key=lambda x: x[\"chunk_id\"])\n",
    "\n",
    "            # Combine all prediction results\n",
    "            combined_predictions = []\n",
    "            total_processing_time = 0\n",
    "            total_items = 0\n",
    "\n",
    "            for result in sorted_results:\n",
    "                combined_predictions.extend(result[\"prediction_results\"])\n",
    "                total_processing_time += result[\"processing_time\"]\n",
    "                total_items += result[\"items_count\"]\n",
    "\n",
    "            avg_processing_time = round((total_processing_time / len(sorted_results)), 2)\n",
    "\n",
    "            # Save to database\n",
    "            job_data = {\n",
    "                \"job_name\": \"batch_ml_prediction\",\n",
    "                \"input_data\": json.dumps({\"chunks\": len(sorted_results), \"total_items\": total_items}),\n",
    "                \"output_data\": json.dumps({\"predictions\": combined_predictions}),\n",
    "                \"processing_time\": avg_processing_time,\n",
    "                \"prediction_count\": total_items,\n",
    "                \"status\": \"completed\",\n",
    "                \"completed_at\": datetime.now(),\n",
    "            }\n",
    "\n",
    "            job = MLPredictionJob(**job_data)\n",
    "            session.add(job)\n",
    "            session.flush()\n",
    "\n",
    "            logger.info(f\"Combined {len(sorted_results)} chunks with {total_items} total predictions\")\n",
    "\n",
    "            return {\n",
    "                \"status\": \"completed\",\n",
    "                \"total_chunks\": len(sorted_results),\n",
    "                \"total_predictions\": total_items,\n",
    "                \"avg_processing_time\": avg_processing_time,\n",
    "                \"job_id\": job.id,\n",
    "                \"predictions\": combined_predictions,\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error combining prediction results: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "@celery_app.task\n",
    "def process_batch_predictions(persons_data: list[dict[str, Any]], chunk_size: int = 10) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a large batch of ML predictions by splitting into chunks and using chord.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    persons_data : list[dict[str, Any]]\n",
    "        List of person data dictionaries for prediction\n",
    "    chunk_size : int, optional\n",
    "        Size of each processing chunk, by default 10\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing batch processing dispatch information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Split data into chunks\n",
    "        chunks = [persons_data[i : i + chunk_size] for i in range(0, len(persons_data), chunk_size)]\n",
    "\n",
    "        # Create a chord: process chunks in parallel, then combine results\n",
    "        job = chord(\n",
    "            group(process_prediction_chunk.s(chunk, i) for i, chunk in enumerate(chunks)),\n",
    "            combine_prediction_results.s(),\n",
    "        )\n",
    "\n",
    "        result = job.apply_async()\n",
    "\n",
    "        logger.info(f\"Dispatched batch prediction job with {len(persons_data)} items in {len(chunks)} chunks\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"dispatched\",\n",
    "            \"total_items\": len(persons_data),\n",
    "            \"chunks\": len(chunks),\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"chord_id\": result.id,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error dispatching batch predictions: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "@celery_app.task(bind=True, base=BaseTask)\n",
    "def process_dlq_message(self, message_data: dict[str, Any]) -> dict[str, Any]:  # noqa: ANN001\n",
    "    \"\"\"\n",
    "    Process a message from the dead letter queue.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    message_data : dict[str, Any]\n",
    "        Message data from DLQ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing DLQ processing results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate the message data\n",
    "        if \"persons\" in message_data:\n",
    "            # Batch message\n",
    "            record = MultiPersonsSchema(**message_data)\n",
    "            message_type = \"batch\"\n",
    "            item_count = len(record.persons)\n",
    "        else:\n",
    "            # Single message\n",
    "            record = PersonSchema(**message_data)\n",
    "            message_type = \"single\"\n",
    "            item_count = 1\n",
    "\n",
    "        # Log DLQ message to database (you might want to create a DLQ table)\n",
    "        logger.warning(f\"Processing DLQ message: {message_type} with {item_count} items\")\n",
    "\n",
    "        # For now, just log the DLQ data - you can extend this to save to a DLQ table\n",
    "        with get_db_session() as session:\n",
    "            job_data = {\n",
    "                \"job_name\": f\"dlq_{message_type}_processing\",\n",
    "                \"input_data\": json.dumps(message_data),\n",
    "                \"output_data\": json.dumps({\"status\": \"dlq_processed\", \"message_type\": message_type}),\n",
    "                \"processing_time\": 0.0,\n",
    "                \"prediction_count\": 0,\n",
    "                \"status\": \"dlq_processed\",\n",
    "                \"completed_at\": datetime.now(),\n",
    "            }\n",
    "\n",
    "            job = MLPredictionJob(**job_data)\n",
    "            session.add(job)\n",
    "            session.flush()\n",
    "\n",
    "            logger.info(f\"DLQ message processed and logged with job_id: {job.id}\")\n",
    "\n",
    "            return {\n",
    "                \"status\": \"dlq_processed\",\n",
    "                \"message_type\": message_type,\n",
    "                \"item_count\": item_count,\n",
    "                \"job_id\": job.id,\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing DLQ message: {e}\")\n",
    "        raise self.retry(exc=e) from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eaaad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769141e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batch-process",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
