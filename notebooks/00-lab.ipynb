{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88083606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Annotated, Any, Generator, Literal, Type, TypeVar\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Visualization\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# NumPy settings\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# Pandas settings\n",
    "pd.options.display.max_rows = 1_000\n",
    "pd.options.display.max_columns = 1_000\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "# Polars settings\n",
    "pl.Config.set_fmt_str_lengths(1_000)\n",
    "pl.Config.set_tbl_cols(n=1_000)\n",
    "pl.Config.set_tbl_rows(n=200)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Black code formatter (Optional)\n",
    "%load_ext lab_black\n",
    "\n",
    "# auto reload imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "custom_theme = Theme({\n",
    "    \"white\": \"#FFFFFF\",  # Bright white\n",
    "    \"info\": \"#00FF00\",  # Bright green\n",
    "    \"warning\": \"#FFD700\",  # Bright gold\n",
    "    \"error\": \"#FF1493\",  # Deep pink\n",
    "    \"success\": \"#00FFFF\",  # Cyan\n",
    "    \"highlight\": \"#FF4500\",  # Orange-red\n",
    "})\n",
    "console = Console(theme=custom_theme)\n",
    "\n",
    "\n",
    "def create_path(path: str | Path) -> None:\n",
    "    \"\"\"\n",
    "    Create parent directories for the given path if they don't exist.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str | Path\n",
    "        The file path for which to create parent directories.\n",
    "\n",
    "    \"\"\"\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def go_up_from_current_directory(*, go_up: int = 1) -> None:\n",
    "    \"\"\"This is used to up a number of directories.\n",
    "\n",
    "    Params:\n",
    "    -------\n",
    "    go_up: int, default=1\n",
    "        This indicates the number of times to go back up from the current directory.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    CONST: str = \"../\"\n",
    "    NUM: str = CONST * go_up\n",
    "\n",
    "    # Goto the previous directory\n",
    "    prev_directory = os.path.join(os.path.dirname(__name__), NUM)\n",
    "    # Get the 'absolute path' of the previous directory\n",
    "    abs_path_prev_directory = os.path.abspath(prev_directory)\n",
    "\n",
    "    # Add the path to the System paths\n",
    "    sys.path.insert(0, abs_path_prev_directory)\n",
    "    print(abs_path_prev_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "142ff501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mac/Desktop/MyProjects/batch-process\n"
     ]
    }
   ],
   "source": [
    "go_up_from_current_directory(go_up=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c95ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<@task: None of None>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "from celery import chord, current_task, group\n",
    "from schemas import DataProcessingSchema\n",
    "from src import create_logger\n",
    "from src.celery import celery_app\n",
    "from src.database import get_db_session\n",
    "from src.database.db_models import BaseTask, DataProcessingJob\n",
    "\n",
    "logger = create_logger(name=\"data_processing\")\n",
    "\n",
    "\n",
    "# Note: When `bind=True`, celery automatically passes the task instance as the first argument\n",
    "# meaning that we need to use `self` and this provides additional functionality like retries, etc\n",
    "@celery_app.task(bind=True, base=BaseTask)\n",
    "def process_data_chunk(self, chunk_data: list[str], chunk_id: int) -> dict[str, Any | None | float | int]:  # noqa: ANN001, ARG001\n",
    "    \"\"\"\n",
    "    Process a chunk of data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_data : list[str]\n",
    "        List of strings to be processed\n",
    "    chunk_id : int\n",
    "        Unique identifier for this chunk\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any | None | float | int]\n",
    "        Dictionary containing processed data, processing time, and item count\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Simulate data processing\n",
    "        processed_data: list[str] = []\n",
    "        total_items: int | None = len(chunk_data)\n",
    "\n",
    "        for i, item in enumerate(chunk_data):\n",
    "            # Update task progress\n",
    "            current_task.update_state(\n",
    "                state=\"PROGRESS\",\n",
    "                meta={\"current\": i + 1, \"total\": total_items, \"chunk_id\": chunk_id},\n",
    "            )\n",
    "\n",
    "            # Simulate processing time\n",
    "            time.sleep(0.9)\n",
    "\n",
    "            if isinstance(item, str):\n",
    "                processed_item = item.upper()\n",
    "\n",
    "            else:\n",
    "                processed_item = item\n",
    "\n",
    "            processed_data.append(processed_item)\n",
    "\n",
    "        processing_time: float | None = time.time() - start_time\n",
    "\n",
    "        logger.info(f\"Processed chunk {chunk_id} with {total_items} items in {processing_time:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"processed_data\": processed_data,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"items_count\": total_items,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing chunk {chunk_id}: {e}\")\n",
    "        raise self.retry(exc=e) from e\n",
    "\n",
    "\n",
    "@celery_app.task\n",
    "def combine_processed_chunks(chunk_results: list[Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Combine results from multiple data processing chunks\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with get_db_session() as session:\n",
    "            # Sort chunks by chunk_id\n",
    "            sorted_results = sorted(chunk_results, key=lambda x: x[\"chunk_id\"])\n",
    "\n",
    "            # Combine all processed data\n",
    "            combined_data: list[str] = []\n",
    "            total_processing_time: int = 0\n",
    "            total_items: int = 0\n",
    "\n",
    "            for result in sorted_results:\n",
    "                combined_data.extend(result[\"processed_data\"])\n",
    "                total_processing_time += result[\"processing_time\"]\n",
    "                total_items += result[\"items_count\"]\n",
    "\n",
    "            avg_processing_time = round((total_processing_time / len(sorted_results)), 2)\n",
    "            # Save to database\n",
    "            data = DataProcessingSchema(\n",
    "                job_name=\"bulk_data_processing\",\n",
    "                input_data=json.dumps({\"chunks\": sorted_results}),\n",
    "                output_data=json.dumps({\"combined_data\": combined_data, \"total_items\": total_items}),\n",
    "                processing_time=avg_processing_time,\n",
    "                status=\"completed\",\n",
    "                completed_at=datetime.now(),\n",
    "            ).model_dump()\n",
    "            job = DataProcessingJob(**data)\n",
    "            session.add(job)\n",
    "            session.flush()\n",
    "\n",
    "            logger.info(f\"Combined {len(sorted_results)} chunks with {total_items} total items\")\n",
    "\n",
    "            return {\n",
    "                \"status\": \"completed\",\n",
    "                \"total_chunks\": len(sorted_results),\n",
    "                \"total_items\": total_items,\n",
    "                \"avg_processing_time\": avg_processing_time,\n",
    "                \"job_id\": job.id,\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error combining chunks: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "@celery_app.task\n",
    "def process_large_dataset(data: list[Any], chunk_size: int = 10) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a large dataset by splitting into chunks and using chord\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Split data into chunks\n",
    "        chunks: list[list[Any]] = [data[i : i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "        # Create a chord: process chunks in parallel, then combine results\n",
    "        job = chord(\n",
    "            group(process_data_chunk.s(chunk, i) for i, chunk in enumerate(chunks)),\n",
    "            combine_processed_chunks.s(),\n",
    "        )\n",
    "\n",
    "        result = job.apply_async()\n",
    "\n",
    "        return {\n",
    "            \"status\": \"dispatched\",\n",
    "            \"total_items\": len(data),\n",
    "            \"chunks\": len(chunks),\n",
    "            \"chord_id\": result.id,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error dispatching large dataset processing: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ecd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_prediction(\n",
    "    record: PersonSchema | MultiPersonsSchema,\n",
    "    model_dict: dict[str, Any],\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Process a single record and return predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    record : PersonSchema | MultiPersonsSchema\n",
    "        Input record containing person or multiple person data.\n",
    "    model_dict : dict[str, Any]\n",
    "        Dictionary containing model and processor objects.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, Any]]\n",
    "        List of dictionaries containing predictions and features.\n",
    "    \"\"\"\n",
    "    data: pl.DataFrame = record_to_dataframe(record)  # type: ignore\n",
    "    # return data\n",
    "    features: npt.NDArray[np.float64] = model_dict[\"processor\"].transform(data)\n",
    "    data_features: pl.DataFrame = pl.DataFrame(\n",
    "        features, schema=model_dict[\"processor\"].get_feature_names_out().tolist()\n",
    "    ).drop([\"num_vars__survived\"])\n",
    "\n",
    "    y_pred: npt.NDArray[np.float64] = model_dict[\"model\"].predict_proba(data_features)[:, 1]\n",
    "    data = data.with_columns(probability=y_pred).with_columns(  # type: ignore\n",
    "        survived=(pl.col(\"probability\") > 0.5).cast(pl.Int64)\n",
    "    )\n",
    "    data_dict: list[dict[str, Any]] = data.to_dicts()\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1613f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cba7aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import delete, insert, select, update\n",
    "\n",
    "from schemas import EmailSchema\n",
    "from src.database.db_models import EmailLog, get_db_session, init_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72537b8c",
   "metadata": {},
   "source": [
    "## [Docs](https://docs.sqlalchemy.org/en/20/orm/queryguide/select.html)\n",
    "\n",
    "### [Insert](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-bulk-insert-statements)\n",
    "\n",
    "- Old API\n",
    "\n",
    "```python\n",
    "with get_db_session() as session:\n",
    "    data_dict = input_data.to_data_model_dict()\n",
    "    record = EmailLog(**data_dict)\n",
    "    session.add(record)\n",
    "    session.flush()\n",
    "    output_data = {key: getattr(record, key) for key in record.output_fields()}\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- New API\n",
    "\n",
    "```py\n",
    "with get_db_session() as session:\n",
    "    data_dict = input_data.to_data_model_dict()\n",
    "    session.execute(insert(EmailLog), [data_dict])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data: EmailSchema = EmailSchema(\n",
    "    recipient=\"marketing@client.com\",\n",
    "    subject=\"Partnership Proposal\",\n",
    "    body=\"We would like to discuss a potential partnership opportunity.\",\n",
    ")\n",
    "console.print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    data_dict = input_data.model_dump()\n",
    "    record = EmailLog(**data_dict)\n",
    "    session.add(record)\n",
    "    session.flush()\n",
    "    output_data = {key: getattr(record, key) for key in record.output_fields()}\n",
    "\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cf44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    statement = session.query(EmailLog).where(EmailLog.created_at < datetime.now())\n",
    "    record = session.execute(statement).scalar_one()\n",
    "    output_data = {key: getattr(record, key) for key in record.output_fields()}\n",
    "\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f60984",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_2: EmailSchema = EmailSchema(\n",
    "    recipient=\"emeka2@example.com\",\n",
    "    subject=\"test!!!\",\n",
    "    body=\"this is an example body\",\n",
    "    status=\"processing\",\n",
    ")\n",
    "input_data_3: EmailSchema = EmailSchema(\n",
    "    recipient=\"john.doe@example.com\",\n",
    "    subject=\"Meeting Reminder\",\n",
    "    body=\"Hi John, just a reminder about our meeting tomorrow at 10 AM.\",\n",
    "    status=\"processing\",\n",
    ")\n",
    "input_data_4: EmailSchema = EmailSchema(\n",
    "    recipient=\"info@company.org\",\n",
    "    subject=\"New Product Launch\",\n",
    "    body=\"Dear valued customer, check out our exciting new product!\",\n",
    "    status=\"sent\",\n",
    "    created_at=datetime(2025, 7, 10, 9, 0, 0),\n",
    "    sent_at=\"2025-07-10T09:05:00\",\n",
    ")\n",
    "console.print((input_data_2, input_data_3, input_data_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321d8cd",
   "metadata": {},
   "source": [
    "### [Bulk Insert](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-bulk-insert-statements)\n",
    "\n",
    "- Old API\n",
    "\n",
    "```py\n",
    "with get_db_session() as session:\n",
    "    data_list: list[dict[str, Any]] = [_data.to_data_model_dict() for _data in (input_data_2, input_data_3, input_data_4)]\n",
    "    session.bulk_insert_mappings(EmailLog, data_list)\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- New API\n",
    "\n",
    "```py\n",
    "with get_db_session() as session:\n",
    "    data_list: list[dict[str, Any]] = [\n",
    "        _data.to_data_model_dict()\n",
    "        for _data in (input_data_2, input_data_3, input_data_4)\n",
    "    ]\n",
    "    session.execute(insert(EmailLog), data_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    data_list: list[dict[str, Any]] = [_data.model_dump() for _data in (input_data_2, input_data_3, input_data_4)]\n",
    "    session.execute(insert(EmailLog), data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81030c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32dc5d45",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab00eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single record\n",
    "with get_db_session() as session:\n",
    "    statement = select(EmailLog).where(EmailLog.id == 1, EmailLog.status == \"pending\")\n",
    "    record = session.execute(statement).scalar_one()\n",
    "    output_data = {key: getattr(record, key) for key in record.output_fields()}\n",
    "\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7778e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all records\n",
    "with get_db_session() as session:\n",
    "    statement = select(EmailLog)\n",
    "    record = session.execute(statement).scalars()\n",
    "\n",
    "    output_data = [{key: getattr(row, key) for key in row.output_fields()} for row in record]\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2e6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "123d4da2",
   "metadata": {},
   "source": [
    "### [Update](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-update-and-delete-with-custom-where-criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22335d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    statement = (\n",
    "        update(EmailLog)\n",
    "        .where(EmailLog.id == 1)\n",
    "        .values(status=\"sent\", sent_at=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    )\n",
    "    # It closes the session and returns None\n",
    "    session.execute(statement)\n",
    "\n",
    "# Verify that the record was updated\n",
    "with get_db_session() as session:\n",
    "    statement = select(EmailLog)\n",
    "    record = session.execute(statement).scalars()\n",
    "\n",
    "    output_data = [{key: getattr(row, key) for key in row.output_fields()} for row in record]\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecabd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90f5ae1",
   "metadata": {},
   "source": [
    "### [Delete](https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-update-and-delete-with-custom-where-criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db_session() as session:\n",
    "    statement = delete(EmailLog).where(EmailLog.id == 2)\n",
    "    # It closes the session and returns None\n",
    "    session.execute(statement)\n",
    "\n",
    "# Verify that the record was updated\n",
    "with get_db_session() as session:\n",
    "    statement = select(EmailLog)\n",
    "    record = session.execute(statement).scalars()\n",
    "\n",
    "    output_data = [{key: getattr(row, key) for key in row.output_fields()} for row in record]\n",
    "\n",
    "console.print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ee946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518f29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import app_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "beat_dict: dict[str, dict[str, Any]] = dict(app_config.celery_config.beat_config.beat_schedule.model_dump().items())\n",
    "\n",
    "# Add the health_check\n",
    "beat_dict[\"health_check\"] = app_config.celery_config.beat_config.health_check.model_dump()\n",
    "\n",
    "\n",
    "console.print(beat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_config.celery_config.beat_config.beat_schedule.model_dump().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07da3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2050af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel, field_serializer\n",
    "\n",
    "\n",
    "class MyModel(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    role: str\n",
    "    salary: float = 0.0\n",
    "    others: Any | None = None\n",
    "\n",
    "    @field_serializer(\"others\")\n",
    "    def serialize(self, value: Any) -> str:\n",
    "        if isinstance(value, datetime):\n",
    "            return value.isoformat()\n",
    "        return json.dumps(value)\n",
    "\n",
    "\n",
    "def my_func(name: str, **kwargs) -> MyModel:\n",
    "    my_dict = {\"name\": name, **kwargs}\n",
    "    return MyModel(**my_dict)\n",
    "\n",
    "\n",
    "result = my_func(\n",
    "    \"Neidu\",\n",
    "    age=30,\n",
    "    role=\"AI Engineer\",\n",
    "    friend=\"None\",\n",
    "    others=[\"Hi\"],\n",
    "    # others=datetime.now(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d0dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.model_dump())\n",
    "\n",
    "json.loads(result.model_dump()[\"others\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import joblib\n",
    "\n",
    "from celery import chord, current_task, group\n",
    "from schemas import ModelOutput, MultiPersonsSchema, MultiPredOutput, PersonSchema\n",
    "from src import PACKAGE_PATH, create_logger\n",
    "from src.celery import celery_app\n",
    "from src.database import get_db_session\n",
    "from src.database.db_models import BaseTask, MLPredictionJob\n",
    "from src.ml.utils import get_batch_prediction, get_prediction\n",
    "\n",
    "logger = create_logger(name=\"ml_prediction\")\n",
    "\n",
    "\n",
    "@celery_app.task(bind=True, base=BaseTask)\n",
    "def process_prediction_chunk(self, persons_data: list[dict[str, Any]], chunk_id: int) -> dict[str, Any]:  # noqa: ANN001\n",
    "    \"\"\"\n",
    "    Process a chunk of ML predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    persons_data : list[dict[str, Any]]\n",
    "        List of person data dictionaries for prediction\n",
    "    chunk_id : int\n",
    "        Unique identifier for this chunk\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing chunk processing results and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Validate input data\n",
    "        multi_persons = MultiPersonsSchema(persons=persons_data)  # type: ignore\n",
    "        total_items = len(multi_persons.persons)\n",
    "\n",
    "        # Load model once for the entire chunk\n",
    "        model_dict_fp: Path = PACKAGE_PATH / \"models/model.pkl\"\n",
    "        with open(model_dict_fp, \"rb\") as f:\n",
    "            model_dict = joblib.load(f)\n",
    "\n",
    "        # Process predictions\n",
    "        prediction_results = []\n",
    "\n",
    "        for i, person in enumerate(multi_persons.persons):\n",
    "            # Update task progress\n",
    "            current_task.update_state(\n",
    "                state=\"PROGRESS\",\n",
    "                meta={\"current\": i + 1, \"total\": total_items, \"chunk_id\": chunk_id},\n",
    "            )\n",
    "\n",
    "            # Make individual prediction\n",
    "            result: ModelOutput = get_prediction(person, model_dict)\n",
    "            prediction_results.append(result.model_dump())\n",
    "\n",
    "        processing_time = time.time() - start_time\n",
    "\n",
    "        logger.info(f\"Processed chunk {chunk_id} with {total_items} predictions in {processing_time:.2f}s\")\n",
    "\n",
    "        return {\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"prediction_results\": prediction_results,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"items_count\": total_items,\n",
    "            \"status\": \"success\",\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing prediction chunk {chunk_id}: {e}\")\n",
    "        raise self.retry(exc=e) from e\n",
    "\n",
    "\n",
    "@celery_app.task\n",
    "def combine_prediction_results(chunk_results: list[dict[str, Any]]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Combine results from multiple prediction chunks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk_results : list[dict[str, Any]]\n",
    "        List of chunk processing results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing combined prediction results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with get_db_session() as session:\n",
    "            # Sort chunks by chunk_id\n",
    "            sorted_results = sorted(chunk_results, key=lambda x: x[\"chunk_id\"])\n",
    "\n",
    "            # Combine all prediction results\n",
    "            combined_predictions = []\n",
    "            total_processing_time = 0\n",
    "            total_items = 0\n",
    "\n",
    "            for result in sorted_results:\n",
    "                combined_predictions.extend(result[\"prediction_results\"])\n",
    "                total_processing_time += result[\"processing_time\"]\n",
    "                total_items += result[\"items_count\"]\n",
    "\n",
    "            avg_processing_time = round((total_processing_time / len(sorted_results)), 2)\n",
    "\n",
    "            # Save to database\n",
    "            job_data = {\n",
    "                \"job_name\": \"batch_ml_prediction\",\n",
    "                \"input_data\": json.dumps({\"chunks\": len(sorted_results), \"total_items\": total_items}),\n",
    "                \"output_data\": json.dumps({\"predictions\": combined_predictions}),\n",
    "                \"processing_time\": avg_processing_time,\n",
    "                \"prediction_count\": total_items,\n",
    "                \"status\": \"completed\",\n",
    "                \"completed_at\": datetime.now(),\n",
    "            }\n",
    "\n",
    "            job = MLPredictionJob(**job_data)\n",
    "            session.add(job)\n",
    "            session.flush()\n",
    "\n",
    "            logger.info(f\"Combined {len(sorted_results)} chunks with {total_items} total predictions\")\n",
    "\n",
    "            return {\n",
    "                \"status\": \"completed\",\n",
    "                \"total_chunks\": len(sorted_results),\n",
    "                \"total_predictions\": total_items,\n",
    "                \"avg_processing_time\": avg_processing_time,\n",
    "                \"job_id\": job.id,\n",
    "                \"predictions\": combined_predictions,\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error combining prediction results: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "@celery_app.task\n",
    "def process_batch_predictions(persons_data: list[dict[str, Any]], chunk_size: int = 10) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a large batch of ML predictions by splitting into chunks and using chord.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    persons_data : list[dict[str, Any]]\n",
    "        List of person data dictionaries for prediction\n",
    "    chunk_size : int, optional\n",
    "        Size of each processing chunk, by default 10\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing batch processing dispatch information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Split data into chunks\n",
    "        chunks = [persons_data[i : i + chunk_size] for i in range(0, len(persons_data), chunk_size)]\n",
    "\n",
    "        # Create a chord: process chunks in parallel, then combine results\n",
    "        job = chord(\n",
    "            group(process_prediction_chunk.s(chunk, i) for i, chunk in enumerate(chunks)),\n",
    "            combine_prediction_results.s(),\n",
    "        )\n",
    "\n",
    "        result = job.apply_async()\n",
    "\n",
    "        logger.info(f\"Dispatched batch prediction job with {len(persons_data)} items in {len(chunks)} chunks\")\n",
    "\n",
    "        return {\n",
    "            \"status\": \"dispatched\",\n",
    "            \"total_items\": len(persons_data),\n",
    "            \"chunks\": len(chunks),\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"chord_id\": result.id,\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error dispatching batch predictions: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "@celery_app.task(bind=True, base=BaseTask)\n",
    "def process_dlq_message(self, message_data: dict[str, Any]) -> dict[str, Any]:  # noqa: ANN001\n",
    "    \"\"\"\n",
    "    Process a message from the dead letter queue.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    message_data : dict[str, Any]\n",
    "        Message data from DLQ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, Any]\n",
    "        Dictionary containing DLQ processing results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate the message data\n",
    "        if \"persons\" in message_data:\n",
    "            # Batch message\n",
    "            record = MultiPersonsSchema(**message_data)\n",
    "            message_type = \"batch\"\n",
    "            item_count = len(record.persons)\n",
    "        else:\n",
    "            # Single message\n",
    "            record = PersonSchema(**message_data)\n",
    "            message_type = \"single\"\n",
    "            item_count = 1\n",
    "\n",
    "        # Log DLQ message to database (you might want to create a DLQ table)\n",
    "        logger.warning(f\"Processing DLQ message: {message_type} with {item_count} items\")\n",
    "\n",
    "        # For now, just log the DLQ data - you can extend this to save to a DLQ table\n",
    "        with get_db_session() as session:\n",
    "            job_data = {\n",
    "                \"job_name\": f\"dlq_{message_type}_processing\",\n",
    "                \"input_data\": json.dumps(message_data),\n",
    "                \"output_data\": json.dumps({\"status\": \"dlq_processed\", \"message_type\": message_type}),\n",
    "                \"processing_time\": 0.0,\n",
    "                \"prediction_count\": 0,\n",
    "                \"status\": \"dlq_processed\",\n",
    "                \"completed_at\": datetime.now(),\n",
    "            }\n",
    "\n",
    "            job = MLPredictionJob(**job_data)\n",
    "            session.add(job)\n",
    "            session.flush()\n",
    "\n",
    "            logger.info(f\"DLQ message processed and logged with job_id: {job.id}\")\n",
    "\n",
    "            return {\n",
    "                \"status\": \"dlq_processed\",\n",
    "                \"message_type\": message_type,\n",
    "                \"item_count\": item_count,\n",
    "                \"job_id\": job.id,\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing DLQ message: {e}\")\n",
    "        raise self.retry(exc=e) from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eaaad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769141e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "batch-process",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
